\input{header.tex}

\title{Context-free Grammars}

\begin{document}

\initAfterBeginDocument{}


\section{Context-free grammar}

\subsection{Definition}

\begin{definition}
A context-free grammar $G$ is a quadruple $(V, \Sigma, R, S)$ where
\begin{itemize}
\item $V$ is an alphabet.
\item $\Sigma \subset V$ is the set of terminals.
\item $S \in V-\Sigma$ is the start symbol.
\item $R \in (V-\Sigma) \times V^*$ is the set of rules.
\end{itemize}
\end{definition}

\begin{definition}[$\rightarrow_G$]
$A \rightarrow_G u \iff (A, u) \in R$.
\end{definition}
\begin{definition}[$\Rightarrow_G$]
$u \Rightarrow_G v$\\
$\iff (\exists x,y,w \in V^*, \exists A \in V - \Sigma,
(u = xAy \wedge v = xwy \wedge A \rightarrow_G w))$
\end{definition}
\begin{definition}[$\Rightarrow_G^L$]
$u \Rightarrow_G^L v$\\
$\iff (\exists x \in \Sigma^*, y, w \in V^*, \exists A \in V - \Sigma,
(u = xAy \wedge v = xwy \wedge A \rightarrow_G w))$
\end{definition}
\begin{definition}[$\Rightarrow_G^n$]
\[ u \Rightarrow_G^0 v \iff u = v \]
\[ \textrm{For } n \ge 1, u \Rightarrow_G^n v \iff (\exists w, u \Rightarrow_G w \wedge w \Rightarrow_G^{n-1} v) \]
\[ u \Rightarrow_G^* v \iff (\exists n, u \Rightarrow_G^n v) \]
\end{definition}
\begin{definition}
For $u \in V^*, L_G(u) = \{w \in \Sigma^*: u \Rightarrow_G^* w \}$.
$L(G) = L_G(S)$.
If $L = L(G)$, $G$ generates $L$ and $L$ is a context-free language.
\end{definition}
\begin{theorem}
For $w \in \Sigma^*$ and $x \in V^*$, $x \Rightarrow_G^* w \iff x \Rightarrow _G^{L*} w$.
\end{theorem}

\subsection{Right-linear grammar}

\begin{definition}
$G$ is a right-linear grammar iff each rule is of the form $P \Rightarrow_G sQ$,
where $P \in V - \Sigma, Q \in V - \Sigma \cup \{e\}$ and $s \in \Sigma^*$.
\end{definition}
\begin{theorem}A language is regular $\iff$ it is generated by a right-linear grammar.\end{theorem}
\begin{proof}[Proof of $\Rightarrow$]
Let $L$ be a regular language. Then it is accepted by the NFA $M = (Q, \Sigma, \Delta, s, F)$.
Let $G = (Q \cup \Sigma, R, s, F)$, where $F = \{q \rightarrow_G e: q \in F\}$
and $R = \{q \rightarrow_G ap: (q, a, p) \in \Delta\}$.
It can be proved that $L(G) = L(M)$.
\end{proof}
\begin{proof}[Proof of $\Leftarrow$]
Let $G = (V, \Sigma, R, S)$ be a right-linear grammar.
Let $M = ((V-\Sigma) \cup \{f\}, \Sigma, \Delta, S, \{f\})$ where
\[ \Delta = \{(P, s, Q): P \rightarrow_G sQ \wedge Q \not\rightarrow_G e \}
\cup \{(P, s, f): P \rightarrow_G sQ \wedge Q \rightarrow_G e \} \]
It can be proved that $L(G) = L(M)$.
\end{proof}

\subsection{Examples}
\begin{itemize}
\item $L(S \rightarrow e \mid aSb) = \{a^nb^n: n \ge 0\}$
\item $L(S \rightarrow e \mid SS \mid (S))$ is the language of balanced parenthesis.
\item $L(S \rightarrow e \mid a \mid b \mid aSa \mid bSb)$ is the language of palindromes.
\item $L(S \rightarrow e \mid aSb \mid bSa \mid SS)$ is the language of strings with
  equal number of occurrences of $a$ and $b$.
\end{itemize}

To prove that $L(G) \subseteq L'$, use induction on the length of derivations
(this is usually easy).
Proving that $L' \subseteq L(G)$ can be done by using induction on the length of input
(this can be hard).

\begin{lemma}
\label{a-b-count-lemma}
Let $f(w)$ be the number of occurrences of $\texttt{a}$ minus the number of occurrences of $\texttt{b}$.
If $f(w) = 0$ and the first and last characters of $w$ are same, then
$w = xy$, where $x, y \in \{a, b\}^+$ and $f(x) = f(y) = 0$.
\end{lemma}
\begin{proof}
It is trivial to prove that $f(u + v) = f(u) + f(v)$.\\
Let $w = czc$ and $|w| = n$, where $c \in \{a, b\}$.\\
Let $w[:k]$ be the $k$-length prefix of $w$.\\
$f(w[:0]) = f(e) = 0, f(w[:1]) = f(c)$.\\
$f(w) = 0 \Rightarrow f(cz) + f(c) = 0 \Rightarrow f(w[:n-1]) = -f(c)$.\\
Since $f(w[:i])$ changes sign for $i \in [1, n-1]$ and $f(w[:i])$ and $f(w[:i+1])$
can only differ by 1, $\exists i \in [1, n-1]$ such that $f(w[:i]) = 0$.\\
Let $w[:i] = x$. Let $w = xy$.
Then $f(x) = 0$ and $f(y) = f(w) - f(x) = 0$.
\end{proof}

\begin{theorem}
$G = (S \rightarrow e \mid aSb \mid bSa \mid SS)$ is the grammar of strings with
equal number of occurrences of $a$ and $b$.
\end{theorem}
\begin{proof}
We will prove by induction that $L' \subseteq L(G)$

Base step: $e \in L(G)$

Inductive step:
Assume that $\forall |x| < n, f(x) = 0 \Rightarrow x \in L(G)$ (induction hypothesis).
Let $|w| = n$ and $f(w) = 0$.

\paragraph{Case 1: First and last characters of $w$ are same.}
If the first and last characters of $w$ are same,
$\exists x, y$ such that $w = xy$ and $1 \le |x|, |y| \le n-1$ and $f(x) = f(y) = 0$ (by lemma \ref{a-b-count-lemma}).
By induction hypothesis, $x, y \in L(G)$.
Therefore, $S \Rightarrow_G SS \Rightarrow_G^* xS \Rightarrow_G^* xy = w$.
Therefore, $w \in L(G)$.

\paragraph{Case 2: $w = axb$.}
$w = axb \Rightarrow f(x) = 0 \Rightarrow x \in L(G)$.
$S \Rightarrow_G aSb \Rightarrow_G^* axb$.
Therefore, $w \in L(G)$.

\paragraph{Case 3: $w = bxa$.}
This is similar to case 2.

In all 3 cases, $w \in L(G)$.
Therefore by mathematical induction, $L' \subseteq L(G)$.
\end{proof}

\section{Parse-trees}

\begin{definition}
A derivation in which we always replace the leftmost non-terminal is a leftmost derivation.
\end{definition}
\begin{theorem}
Every derivation has a unique parse tree associated with it.
Every parse tree has a unique leftmost derivation.
\end{theorem}
\begin{proof}
A derivation can be used to build the parse tree step-by-step.
The string obtained after $k$ steps in the derivation will match
the yield of the parse tree after $k$ steps.

In the beginning, the tree has a single node containing the start symbol.

In the $k^{\textrm{th}}$ step, when a rule application happens,
a certain non-terminal in the string gets replaced.
Find the corresponding symbol in leaf nodes of the parse tree
and expand that node.

Therefore, corresponding to a derivation,
we get a persistent-parse-tree (look up the definition of `persistent data structure').
The last time-slice is the parse tree we want.
Since the construction is deterministic, the parse tree is unique.

We can also do the reverse:
Given a parse tree, we can create a persistent parse tree out of it.
The first time-slice will be a single node containing the start symbol.

To get the $k^{\textrm{th}}$ time-slice from the $(k+1)^{\textrm{th}}$ time-slice,
expand a leaf node containing a non-terminal.
The rule to be used for expansion has to be looked up from the parse tree.

This way, we can get a sequence of parse-trees where each one is obtained
from the next by expanding a single non-terminal leaf node.
The yields of the parse trees will give the derivation of the string.

If we always expand the leftmost non-terminal in the parse-tree slice,
the algorithm will become deterministic, so we will obtain a unique derivation.
This derivation is a leftmost derivation.
\end{proof}

\begin{definition}
If multiple distinct parse trees exist for a string in a grammar, that string is said to be `ambiguous' for that grammar.
A grammar which has an ambiguous string is an ambiguous grammar.
A language for which all grammars are ambiguous is `inherently ambiguous'.
\end{definition}

\section{Pushdown Automata}

\begin{definition}
A pushdown automaton $M$ is a tuple $(K, \Sigma, \Gamma, \Delta, s, F)$ where
\begin{itemize}
\item $K$ is the set of states.
\item $\Sigma$ is the input alphabet.
\item $\Gamma$ is the stack alphabet.
\item $s \in K$ is the start state.
\item $F \subseteq K$ are the final states.
\item $\Delta \subseteq (K \times (\Sigma?) \times \Gamma^*) \times (K \times \Gamma^*)$
    (finite subset) is the transition function.
    $((p, a, \beta), (q, \gamma)) \in \Delta$ means `when in state $p$, input symbol is $a$
    and top of the stack is $\beta$, go to state $q$, pop $\beta$ and push $\gamma$'.
    Top of the stack being $\beta$ means that $\beta$ is a substring of the stack when the stack is read top-to-bottom.
    Pushing $\gamma$ means first push its last symbol, then its second-last symbol, and so on.
\end{itemize}
\end{definition}
\begin{definition}
A configuration of a pushdown automaton $M$ is an element $(q, w, x) \in (K \times \Sigma^* \times \Gamma^*)$
which means that $M$ is in state $q$, input $w$ is remaining to be read and the stack content is $x$
(left-to-right in $x$ means top-to-bottom in stack).
\end{definition}
\begin{definition}
$(p, x, u) \vdash_M (q, y, v) \iff $ there is a transition
which can take $M$ from $(p, x, u)$ to $(q, y, v)$.
$\vdash_M^*$ is the reflexive-transitive closure of $\vdash_M$.
\end{definition}
\begin{definition}
$w \in L(M) \iff (\exists f \in F, (s, w, e) \vdash_M (f, e, e))$.
\end{definition}

\section{Pushdown automata and context-free grammars}

\subsection{Incremental derivation}

\begin{definition}
Let $G = (V, \Sigma, R, S)$ be a grammar.\\
For $u_1, u_2 \in \Sigma^*$, $A \in V$ and $v_1, v_2 \in V^*$,
$(u_1, Av_1) \Rightarrow_G (u_2, v_2)$\\
$\iff \begin{cases}
u_2 = u_1A \wedge v_2 = v_1 & \textrm{ if } A \in \Sigma\\
u_2 = u_1 \wedge
(\exists \beta \in V^*, A \rightarrow_G \beta \wedge v_2 = \beta v_1)
& \textrm{ if } A \in V - \Sigma \end{cases}$
\end{definition}

For $x \in V^*$ and $w \in \Sigma^*$, if $(e, v) \Rightarrow_G^* (w, e)$,
then the sequence of $\Rightarrow_G$ operations
is called an incremental derivation of $w$ from $v$.

\begin{lemma}
$((u_1, v_1) \Rightarrow_G (u_2, v_2)) \implies (u_1v_1 = u_2v_2 \vee u_1v_1 \Rightarrow_G^L u_2v_2)$
\end{lemma}
\begin{lemma}
For $x \in \Sigma^*$, $A \in V-\Sigma$ and $y, \beta \in V^*$,\\
$xAy \Rightarrow_G^L x\beta y \implies (x, Ay) \Rightarrow_G (x, \beta y)$.
\end{lemma}
\begin{theorem}
For $w \in \Sigma^*$,
$(\gamma \Rightarrow_G^* w) \iff ((e, \gamma) \Rightarrow_G^* (w, e))$.
\end{theorem}
Therefore, $w \in L(G)$ iff an incremental derivation of $w$ exists from $S$,
i.e. $(e, S) \Rightarrow_G^* (w, e)$.

\subsection{Context-free grammar to pushdown automaton}

Let $G = (V, \Sigma, R, S)$ be a context-free grammar.
Let $M = (\{s, f\}, \Sigma, V, \Delta, s, \{f\})$ be a pushdown automaton.

Denote by $(f, u, v, x)_w$ the configuration of $M$ on input $w$ when it is at state $f$,
has read input $u$, has input $v$ remaining to be read and contains $x$ on the stack.

\begin{theorem}
$(f, u_1, v_1, x)_w \vdash_M (f, u_2, v_2, x)_w \iff (u_1, x_1) \Rightarrow_G (u_2, x_2)$.
\end{theorem}
\begin{theorem}$L(M) = L(G)$.\end{theorem}
\begin{proof}
\begin{align*}
& w \in L(G)
\\ &\iff (e, S) \Rightarrow_G^* (w, e)
\\ &\iff (f, e, w, S)_w \vdash_M (f, w, e, e)_w
\\ &\iff w \in L(M)
\end{align*}
\end{proof}

\subsection{Simple pushdown automata}

A pushdown automaton $M$ is simple iff all of the following are true:
\begin{itemize}
\item There is no transition to the start state.
    There is a single transition from the start state
    and that transition does not read the input tape.
\item There is a single final state.
    Transitions to the final state do not read the input tape.
\item In every transition, $M$ either pushes a single symbol or pops a single symbol or does nothing to the stack.
\item $M$'s stack is never empty except at the start state or in the final state where it should be empty.
\item Every transition (except the one from the start state) consults the stack.
\end{itemize}

This implies that in a simple pushdown automaton,
the start state always has to push a single symbol to the stack.
We call it the `bottom marker' and denote it by $Z$.
Also, $Z$ is always present in the stack except at the start state or the final state.

A pushdown automaton $M = (Q, \Sigma, \Gamma, \Delta, s, F)$
can always be transformed to be simple. This is how:
\begin{itemize}
\item Introduce a new stack symbol $Z$.
\item Add a new start state $s'$ and the transition $((s', e, e), (s, Z))$.
\item Add a new final state $f'$ and the transition $((f, e, Z), (f', e))$ for all $f \in F$.
\item $\forall ((p, a, \beta), (q, \gamma)) \in \Delta$, add new states and transitions which
    first iteratively pop symbols of $\beta$ and then iteratively push symbols of $\gamma$.
    The stack will never get empty because no pop can remove $Z$.
\item Replace $((p, a, e), (q, b))$ by the transitions $((p, a, c), (q, cb))$ for every $c \in \Sigma$.
\end{itemize}

\subsection{Pushdown automaton to context-free grammar}

Let $M = (Q, \Sigma, \Gamma \cup \{Z\}, \Delta, s, \{f\})$ be a simple pushdown automaton.\\
Let $((s,e,e), (s',Z)) \in \Delta$ and $((f',e,Z), (f,e)) \in \Delta$.

Let $G = (V, \Sigma, R, S)$ where
$V = \Sigma \cup \{S\} \cup \{\langle q, A, p \rangle: p, q \in Q \wedge A \in \Gamma \cup \{e, Z\}\}$.

There are 4 kinds of rules in $R$:
\begin{itemize}
\item $S \rightarrow \langle s, Z, f \rangle$.
\item For $q \in Q$: $\langle q, e, q \rangle \rightarrow e$.
\item For every $p \in Q$ and every stack pop/no-op $((q,a,B), (r,C))$ ($C \in \{e, B\}$):\\
$\langle q, B, p \rangle \rightarrow a \langle r, C, p \rangle$.
\item For every $p, p' \in Q$ and every stack push $((q,a,B), (r,CB))$:\\
$\langle q, B, p \rangle \rightarrow a \langle r, C, p' \rangle \langle p', B, p \rangle$.
\end{itemize}

\begin{lemma}(Can be proved using induction)\\
$\langle p, A, q \rangle \Rightarrow_G^* x \iff (p, A, x) \vdash_M^* (q, e, e)$
\end{lemma}
\begin{theorem}$L(G) = L(M)$\end{theorem}
\begin{proof}
\begin{align*}
& w \in L(G)
\\ &\iff \langle s', Z, f \rangle \Rightarrow_G^* w
\\ &\iff (s', w, Z) \vdash_M^* (f, e, e)
\\ &\iff w \in L(M)
\end{align*}
\end{proof}

\end{document}
