\input{header.tex}

\usepackage{algorithm}
\usepackage{algpseudocode}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\title{Context-free Grammars}

\newcommand*{\garrow}[1]{\rightarrow_{#1}}
\newcommand*{\gArrow}[1]{\Rightarrow_{#1}}
\newcommand*{\gArrowL}[1]{\overset{L}{\Rightarrow}_{#1}}
\newcommand*{\gArrowR}[1]{\overset{R}{\Rightarrow}_{#1}}

\begin{document}

\initAfterBeginDocument{}


\section{Context-free grammar}

\subsection{Definition}

\begin{definition}
A context-free grammar $G$ is a quadruple $(V, \Sigma, R, S)$ where
\begin{itemize}
\item $V$ is an alphabet.
\item $\Sigma \subset V$ is the set of terminals.
\item $S \in V-\Sigma$ is the start symbol.
\item $R \in (V-\Sigma) \times V^*$ is the set of rules.
\end{itemize}
\end{definition}

\begin{definition}
$|R|$ is the number of rules in $G$.
$|G|$ is the sum of length of all rules in $G$.
\end{definition}

\begin{definition}
$A \garrow{G} u \iff (A, u) \in R$.
\end{definition}
\begin{definition}
$u \gArrow{G} v$\\
$\iff (\exists x,y,w \in V^*, \exists A \in V - \Sigma,
(u = xAy \wedge v = xwy \wedge A \garrow{G} w))$
\end{definition}
\begin{definition}
$u \gArrowL{G} v$\\
$\iff (\exists x \in \Sigma^*, y, w \in V^*, \exists A \in V - \Sigma,
(u = xAy \wedge v = xwy \wedge A \garrow{G} w))$
\end{definition}
\begin{definition}
$u \gArrowR{G} v$\\
$\iff (\exists x, w \in V^*, \exists y \in \Sigma^*, \exists A \in V - \Sigma,
(u = xAy \wedge v = xwy \wedge A \garrow{G} w))$
\end{definition}
\begin{definition}[$\gArrow{G}^n$]
\[ u \gArrow{G}^0 v \iff u = v \]
\[ \textrm{For } n \ge 1, u \gArrow{G}^n v \iff (\exists w, u \gArrow{G} w \wedge w \gArrow{G}^{n-1} v) \]
\[ u \gArrow{G}^* v \iff (\exists n, u \gArrow{G}^n v) \]
\end{definition}
\begin{definition}
For $u \in V^*, L_G(u) = \{w \in \Sigma^*: u \gArrow{G}^* w \}$.
$L(G) = L_G(S)$.
If $L = L(G)$, $G$ generates $L$ and $L$ is a context-free language.
\end{definition}
\begin{theorem}
For $w \in \Sigma^*$ and $x \in V^*$, $x \gArrow{G}^* w \iff x \Rightarrow _G^{L*} w$.
\end{theorem}

\subsection{Right-linear grammar}

\begin{definition}
$G$ is a right-linear grammar iff each rule is of the form $P \gArrow{G} sQ$,
where $P \in V - \Sigma, Q \in V - \Sigma \cup \{e\}$ and $s \in \Sigma^*$.
\end{definition}
\begin{theorem}A language is regular $\iff$ it is generated by a right-linear grammar.\end{theorem}
\begin{proof}[Proof of $\Rightarrow$]
Let $L$ be a regular language. Then it is accepted by the NFA $M = (Q, \Sigma, \Delta, s, F)$.
Let $G = (Q \cup \Sigma, R, s, F)$, where $F = \{q \garrow{G} e: q \in F\}$
and $R = \{q \garrow{G} ap: (q, a, p) \in \Delta\}$.
It can be proved that $L(G) = L(M)$.
\end{proof}
\begin{proof}[Proof of $\Leftarrow$]
Let $G = (V, \Sigma, R, S)$ be a right-linear grammar.
Let $M = ((V-\Sigma) \cup \{f\}, \Sigma, \Delta, S, \{f\})$ where
\[ \Delta = \{(P, s, Q): P \garrow{G} sQ \wedge Q \not\garrow{G} e \}
\cup \{(P, s, f): P \garrow{G} sQ \wedge Q \garrow{G} e \} \]
It can be proved that $L(G) = L(M)$.
\end{proof}

\subsection{Examples}
\begin{itemize}
\item $L(S \rightarrow e \mid aSb) = \{a^nb^n: n \ge 0\}$
\item $L(S \rightarrow e \mid SS \mid (S))$ is the language of balanced parenthesis.
\item $L(S \rightarrow e \mid a \mid b \mid aSa \mid bSb)$ is the language of palindromes.
\item $L(S \rightarrow e \mid aSb \mid bSa \mid SS)$ is the language of strings with
  equal number of occurrences of $a$ and $b$.
\end{itemize}

To prove that $L(G) \subseteq L'$, use induction on the length of derivations
(this is usually easy).
Proving that $L' \subseteq L(G)$ can be done by using induction on the length of input
(this can be hard).

\begin{lemma}
\label{a-b-count-lemma}
Let $f(w)$ be the number of occurrences of $\texttt{a}$ minus the number of occurrences of $\texttt{b}$.
If $f(w) = 0$ and the first and last characters of $w$ are same, then
$w = xy$, where $x, y \in \{a, b\}^+$ and $f(x) = f(y) = 0$.
\end{lemma}
\begin{proof}
It is trivial to prove that $f(u + v) = f(u) + f(v)$.\\
Let $w = czc$ and $|w| = n$, where $c \in \{a, b\}$.\\
Let $w[:k]$ be the $k$-length prefix of $w$.\\
$f(w[:0]) = f(e) = 0, f(w[:1]) = f(c)$.\\
$f(w) = 0 \Rightarrow f(cz) + f(c) = 0 \Rightarrow f(w[:n-1]) = -f(c)$.\\
Since $f(w[:i])$ changes sign for $i \in [1, n-1]$ and $f(w[:i])$ and $f(w[:i+1])$
can only differ by 1, $\exists i \in [1, n-1]$ such that $f(w[:i]) = 0$.\\
Let $w[:i] = x$. Let $w = xy$.
Then $f(x) = 0$ and $f(y) = f(w) - f(x) = 0$.
\end{proof}

\begin{theorem}
$G = (S \rightarrow e \mid aSb \mid bSa \mid SS)$ is the grammar of strings with
equal number of occurrences of $a$ and $b$.
\end{theorem}
\begin{proof}
We will prove by induction that $L' \subseteq L(G)$

Base step: $e \in L(G)$

Inductive step:
Assume that $\forall |x| < n, f(x) = 0 \Rightarrow x \in L(G)$ (induction hypothesis).
Let $|w| = n$ and $f(w) = 0$.

\paragraph{Case 1: First and last characters of $w$ are same.}
If the first and last characters of $w$ are same,
$\exists x, y$ such that $w = xy$ and $1 \le |x|, |y| \le n-1$ and $f(x) = f(y) = 0$ (by lemma \ref{a-b-count-lemma}).
By induction hypothesis, $x, y \in L(G)$.
Therefore, $S \gArrow{G} SS \gArrow{G}^* xS \gArrow{G}^* xy = w$.
Therefore, $w \in L(G)$.

\paragraph{Case 2: $w = axb$.}
$w = axb \Rightarrow f(x) = 0 \Rightarrow x \in L(G)$.
$S \gArrow{G} aSb \gArrow{G}^* axb$.
Therefore, $w \in L(G)$.

\paragraph{Case 3: $w = bxa$.}
This is similar to case 2.

In all 3 cases, $w \in L(G)$.
Therefore by mathematical induction, $L' \subseteq L(G)$.
\end{proof}

\section{Parse-trees}

\begin{definition}
A derivation in which we always replace the leftmost non-terminal is a leftmost derivation.
\end{definition}
\begin{theorem}
Every derivation has a unique parse tree associated with it.
Every parse tree has a unique leftmost derivation.
\end{theorem}
\begin{proof}
A derivation can be used to build the parse tree step-by-step.
The string obtained after $k$ steps in the derivation will match
the yield of the parse tree after $k$ steps.

In the beginning, the tree has a single node containing the start symbol.

In the $k^{\textrm{th}}$ step, when a rule application happens,
a certain non-terminal in the string gets replaced.
Find the corresponding symbol in leaf nodes of the parse tree
and expand that node.

Therefore, corresponding to a derivation,
we get a persistent-parse-tree (look up the definition of `persistent data structure').
The last time-slice is the parse tree we want.
Since the construction is deterministic, the parse tree is unique.

We can also do the reverse:
Given a parse tree, we can create a persistent parse tree out of it.
The first time-slice will be a single node containing the start symbol.

To get the $k^{\textrm{th}}$ time-slice from the $(k+1)^{\textrm{th}}$ time-slice,
expand a leaf node containing a non-terminal.
The rule to be used for expansion has to be looked up from the parse tree.

This way, we can get a sequence of parse-trees where each one is obtained
from the next by expanding a single non-terminal leaf node.
The yields of the parse trees will give the derivation of the string.

If we always expand the leftmost non-terminal in the parse-tree slice,
the algorithm will become deterministic, so we will obtain a unique derivation.
This derivation is a leftmost derivation.
\end{proof}

\begin{definition}
If multiple distinct parse trees exist for a string in a grammar, that string is said to be `ambiguous' for that grammar.
A grammar which has an ambiguous string is an ambiguous grammar.
A language for which all grammars are ambiguous is `inherently ambiguous'.
\end{definition}

\section{Pushdown Automata}

\begin{definition}
A pushdown automaton $M$ is a tuple $(K, \Sigma, \Gamma, \Delta, s, F)$ where
\begin{itemize}
\item $K$ is the set of states.
\item $\Sigma$ is the input alphabet.
\item $\Gamma$ is the stack alphabet.
\item $s \in K$ is the start state.
\item $F \subseteq K$ are the final states.
\item $\Delta \subseteq (K \times (\Sigma?) \times \Gamma^*) \times (K \times \Gamma^*)$
    (finite subset) is the transition function.
    $((p, a, \beta), (q, \gamma)) \in \Delta$ means `when in state $p$, input symbol is $a$
    and top of the stack is $\beta$, go to state $q$, pop $\beta$ and push $\gamma$'.
    Top of the stack being $\beta$ means that $\beta$ is a substring of the stack when the stack is read top-to-bottom.
    Pushing $\gamma$ means first push its last symbol, then its second-last symbol, and so on.
\end{itemize}
\end{definition}
\begin{definition}
A configuration of a pushdown automaton $M$ is an element $(q, w, x) \in (K \times \Sigma^* \times \Gamma^*)$
which means that $M$ is in state $q$, input $w$ is remaining to be read and the stack content is $x$
(left-to-right in $x$ means top-to-bottom in stack).
\end{definition}
\begin{definition}
$(p, x, u) \vdash_M (q, y, v) \iff $ there is a transition
which can take $M$ from $(p, x, u)$ to $(q, y, v)$.
$\vdash_M^*$ is the reflexive-transitive closure of $\vdash_M$.
\end{definition}
\begin{definition}
$w \in L(M) \iff (\exists f \in F, (s, w, e) \vdash_M (f, e, e))$.
\end{definition}

\section{Pushdown automata and context-free grammars}

\subsection{Incremental derivation}

\begin{definition}
Let $G = (V, \Sigma, R, S)$ be a grammar.\\
For $u_1, u_2 \in \Sigma^*$, $A \in V$ and $v_1, v_2 \in V^*$,
$(u_1, Av_1) \gArrow{G} (u_2, v_2)$\\
$\iff \begin{cases}
u_2 = u_1A \wedge v_2 = v_1 & \textrm{ if } A \in \Sigma\\
u_2 = u_1 \wedge
(\exists \beta \in V^*, A \garrow{G} \beta \wedge v_2 = \beta v_1)
& \textrm{ if } A \in V - \Sigma \end{cases}$
\end{definition}

For $x \in V^*$ and $w \in \Sigma^*$, if $(e, v) \gArrow{G}^* (w, e)$,
then the sequence of $\gArrow{G}$ operations
is called an incremental derivation of $w$ from $v$.

\begin{lemma}
$((u_1, v_1) \gArrow{G} (u_2, v_2)) \implies (u_1v_1 = u_2v_2 \vee u_1v_1 \gArrowL{G} u_2v_2)$
\end{lemma}
\begin{lemma}
For $x \in \Sigma^*$, $A \in V-\Sigma$ and $y, \beta \in V^*$,\\
$xAy \gArrowL{G} x\beta y \implies (x, Ay) \gArrow{G} (x, \beta y)$.
\end{lemma}
\begin{theorem}
For $w \in \Sigma^*$,
$(\gamma \gArrow{G}^* w) \iff ((e, \gamma) \gArrow{G}^* (w, e))$.
\end{theorem}
Therefore, $w \in L(G)$ iff an incremental derivation of $w$ exists from $S$,
i.e. $(e, S) \gArrow{G}^* (w, e)$.

\subsection{Context-free grammar to pushdown automaton}

Let $G = (V, \Sigma, R, S)$ be a context-free grammar.
Let $M = (\{s, f\}, \Sigma, V, \Delta, s, \{f\})$ be a pushdown automaton.
$\Delta$ contains the following transitions:
\begin{itemize}
\item $((s, e, e), (f, S))$
\item $((f, a, a), (f, e))$ for all $a \in \Sigma$.
\item $((f, e, A), (f, \beta))$ for all $A \garrow{G} \beta \in R$.
\end{itemize}

Denote by $(f, u, v, x)_w$ the configuration of $M$ on input $w$ when it is at state $f$,
has read input $u$, has input $v$ remaining to be read and contains $x$ on the stack.

\begin{theorem}
$(f, u_1, v_1, x_1)_w \vdash_M (f, u_2, v_2, x_2)_w \iff (u_1, x_1) \gArrow{G} (u_2, x_2)$.
\end{theorem}
\begin{theorem}$L(M) = L(G)$.\end{theorem}
\begin{proof}
\begin{align*}
& w \in L(G)
\\ &\iff (e, S) \gArrow{G}^* (w, e)
\\ &\iff (f, e, w, S)_w \vdash_M^* (f, w, e, e)_w
\\ &\iff w \in L(M)
\end{align*}
\end{proof}

\subsection{Simple pushdown automata}

A pushdown automaton $M$ is simple iff all of the following are true:
\begin{itemize}
\item There is no transition to the start state.
    There is a single transition from the start state
    and that transition does not read the input tape.
\item There is a single final state.
    Transitions to the final state do not read the input tape.
\item In every transition, $M$ either pushes a single symbol or pops a single symbol or does nothing to the stack.
\item $M$'s stack is never empty except at the start state or in the final state where it should be empty.
\item Every transition (except the one from the start state) consults the stack.
\end{itemize}

This implies that in a simple pushdown automaton,
the start state always has to push a single symbol to the stack.
We call it the `bottom marker' and denote it by $Z$.
Also, $Z$ is always present in the stack except at the start state or the final state.

A pushdown automaton $M = (Q, \Sigma, \Gamma, \Delta, s, F)$
can always be transformed to be simple. This is how:
\begin{itemize}
\item Introduce a new stack symbol $Z$.
\item Add a new start state $s'$ and the transition $((s', e, e), (s, Z))$.
\item Add a new final state $f'$ and the transition $((f, e, Z), (f', e))$ for all $f \in F$.
\item $\forall ((p, a, \beta), (q, \gamma)) \in \Delta$, add new states and transitions which
    first iteratively pop symbols of $\beta$ and then iteratively push symbols of $\gamma$.
    The stack will never get empty because no pop can remove $Z$.
\item Replace $((p, a, e), (q, b))$ by the transitions $((p, a, c), (q, cb))$ for every $c \in \Sigma$.
\end{itemize}

\subsection{Pushdown automaton to context-free grammar}

Let $M = (Q, \Sigma, \Gamma \cup \{Z\}, \Delta, s, \{f\})$ be a simple pushdown automaton.\\
Let $((s,e,e), (s',Z)) \in \Delta$ and $((f',e,Z), (f,e)) \in \Delta$.

Let $G = (V, \Sigma, R, S)$ where
$V = \Sigma \cup \{S\} \cup \{\langle q, A, p \rangle: p, q \in Q \wedge A \in \Gamma \cup \{e, Z\}\}$.

There are 4 kinds of rules in $R$:
\begin{itemize}
\item $S \rightarrow \langle s, Z, f \rangle$.
\item For $q \in Q$: $\langle q, e, q \rangle \rightarrow e$.
\item For every $p \in Q$ and every stack pop/no-op $((q,a,B), (r,C))$ ($C \in \{e, B\}$):\\
$\langle q, B, p \rangle \rightarrow a \langle r, C, p \rangle$.
\item For every $p, p' \in Q$ and every stack push $((q,a,B), (r,CB))$:\\
$\langle q, B, p \rangle \rightarrow a \langle r, C, p' \rangle \langle p', B, p \rangle$.
\end{itemize}

\begin{lemma}(Can be proved using induction)\\
$\langle p, A, q \rangle \gArrow{G}^* x \iff (p, A, x) \vdash_M^* (q, e, e)$
\end{lemma}
\begin{theorem}$L(G) = L(M)$\end{theorem}
\begin{proof}
\begin{align*}
& w \in L(G)
\\ &\iff \langle s', Z, f \rangle \gArrow{G}^* w
\\ &\iff (s', w, Z) \vdash_M^* (f, e, e)
\\ &\iff w \in L(M)
\end{align*}
\end{proof}

\section{Closure properties}

Let $G_1 = (V_1, \Sigma_1, R_1, S_1)$ and $G_2 = (V_2, \Sigma_2, R_2, S_2)$
be context-free grammars. Since renaming non-terminals does not affect a grammar,
we can assume that $V_1 - \Sigma_1$ and $V_2 - \Sigma_2$ are disjoint.

$L(G_1) \cup L(G_2)$
$= L((V_1 \cup V_2 \cup \{S\}, \Sigma_1 \cup \Sigma_2,
R_1 \cup R_2 \cup \{S \rightarrow S_1 | S_2 \}, S))$

$L(G_1)L(G_2)$
$= L((V_1 \cup V_2 \cup \{S\}, \Sigma_1 \cup \Sigma_2,
R_1 \cup R_2 \cup \{S \rightarrow S_1S_2 \}, S))$

$L(G_1)^* = L((V_1 \cup \{S\}, \Sigma_1,
R_1 \cup \{S \rightarrow e | S_1S \}, S))$

\begin{theorem}
Intersection of a regular language with a context-free language is context-free.
\end{theorem}
A pushdown automaton for the intersection can be obtained by taking the
cross product of the states of the DFA of the regular language
and the states of the pushdown automaton of the context-free language.

\section{Pumping Theorem}

\begin{definition}
The fanout of a grammar $G$, denoted as $\phi(G)$, is the maximum RHS length in its rules.
\end{definition}
\begin{lemma}
Let $G = (V, \Sigma, R, S)$ be a context-free grammar.
For $w \in L(G)$, $|w| > \phi(G)^h$ implies that the parse tree of $w$ has height greater than $h$.
\end{lemma}
\begin{theorem}[Pumping theorem]
Let $G = (V, \Sigma, R, S)$ be a context-free grammar.
Then $\forall w \in L(G), |w| > \phi(G)^{|V - \Sigma|} \implies$\\
$(\exists u, v, x, y, z \in \Sigma^*, w = uvxyz \wedge vy \neq e \wedge$\\
$(\forall i \ge 0, uv^nxy^nz \in L(G)))$.
\end{theorem}

\begin{corollary}
$L = \{a^nb^nc^n: n \ge 0\}$ is not context-free.
\end{corollary}
\begin{proof}
Let $w = a^nb^nc^n$ where $n > \phi(G)^{|V-\Sigma|}/3$.
Let $w = uvxyz$ such that $vy \neq 0$.
If $vy$ has all occurrences of all of $\{a, b, c\}$,
then $v$ or $y$ has occurrences of 2 of $\{a, b, c\}$.
Therefore, $uv^kxy^kz$ will have characters in the wrong order for $k \ge 2$.
If $vy$ does not have a certain character,
then $uv^kxy^kz$ will have unequal number of characters for $k \neq 1$.
This contradicts the pumping theorem, so $L$ is not context-free.
\end{proof}

\begin{theorem}[Corollary of Parikh's Theorem]
Let $L$ be a language over a single character.
Then $L$ is context-free iff $L$ is regular.
\end{theorem}

\begin{theorem}
Context-free languages are not closed under intersection or complementation.
\end{theorem}
\begin{proof}
$L_1 = \{a^mb^nc^n: m, n \ge 0\}$ and $L_2 = \{a^mb^mc^n: m, n \ge 0\}$ are context-free.
But $L_1 \cap L_2 = \{a^nb^nc^n: n \ge 0\}$ is not context-free.
Complementation cannot be closed under context-free languages,
because $L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}$.
\end{proof}

\section{Algorithms for context-free grammars}

\subsection{Finding erasable non-terminals}

\begin{definition}
A non-terminal $A$ is erasable iff $A \gArrow{G}^* e$.
\end{definition}

To find the set of erasable non-terminals, follow this algorithm:

\begin{algorithm}[H]
\caption{Finding all erasable non-terminals in $G = (V, \Sigma, R, S)$}
\label{algo-erasable-non-terminals}
\begin{algorithmic}
\State $\mathcal{E} = \{\}$
\Comment{Implement as a bitset on $V - \Sigma$.}
\Do
    \State $\mathcal{E}' = \mathcal{E}$
    \For{$A \garrow{G} \beta \in R$ where $A \not\in \mathcal{E}$}
        \If{$\beta \in \mathcal{E}^*$}
            \State $\mathcal{E}.\operatorname{add}(A)$
        \EndIf
    \EndFor
\doWhile{$\mathcal{E}' \neq \mathcal{E}$}
\end{algorithmic}
\end{algorithm}

Running time: $O(|V-\Sigma||G|)$.
Auxiliary space: $\Theta(|V-\Sigma|)$.

\subsection{Finding unit derivability}

\begin{definition}
$B \in V$ is unit-derivable from $A$ iff $A \gArrow{G}^* B$.
$\operatorname{D}(A) = \{B \in V: A \gArrow{G}^* B\}$.
\end{definition}

Algorithm to find $D(A)$ for all $A \in V$,
if the grammar $G = (V, \Sigma, R, S)$ has no erasable non-terminals:
\begin{itemize}
\item Make a directed graph where $V$ is the set of vertices.
    Make an edge from $A$ to $B$ iff $A \garrow{G} B \in R$.
\item Apply depth-first search from every $A \in V$.
    Add every vertex encountered during depth-first search to $\operatorname{D}(A)$.
\end{itemize}

Running time: $O(|V-\Sigma|\min(|R|, |V||V-\Sigma|))$.\\
Auxiliary space: $O(\min(|R|, |V||V-\Sigma|) + |V-\Sigma|)$.\\
Output size: $O(|V-\Sigma||V|)$.

\subsection{Chomsky normal form}

\begin{definition}
A grammar $G = (V, \Sigma, R, S)$ is in Chomsky normal form iff every rule is of one of these forms:
\begin{itemize}
\item $A \garrow{G} BC$ where $A \in V - \Sigma$ and $B, C \in V - \{S\}$.
\item $A \garrow{G} a$ where $A \in V - \Sigma$ and $a \in \Sigma$.
\item $S \garrow{G} e$
\end{itemize}
\end{definition}

The following algorithm transforms a grammar $G$ into Chomsky normal form grammar $G'$.
The algorithm proceeds in stages, and in each state the language of the grammar remains unchanged:
\begin{enumerate}
\item Eliminate $S$ from RHS: Change the start symbol to $S'$ and add the rule $S' \garrow{G} S$.
\item Eliminate long rules, i.e. rules of the form $A \garrow{G} B_1B_2\ldots B_n$ where $B_i \in V$ and $n \ge 3$.
\item Eliminate $e-$rules, i.e. rules of the form $A \garrow{G} e$.
\item Eliminate unit rules, i.e. rules of the form $A \garrow{G} B$, where $B \in V$.
\end{enumerate}

\subsubsection{Eliminating long rules}

Replace each rule of the form $A \garrow{G} B_1B_2\ldots B_n$ by the rules
$A \garrow{G} B_1A_1 \;,\; A_1 \garrow{G} B_2A_2 \;,\; \ldots \;,\;
A_{n-3} \garrow{G} B_{n-2}A_{n-2} \;,\; A_{n-2} \garrow{G} B_{n-1}B_n$.

\begin{itemize}
\item Running time: $\Theta(|G|)$.
\item Auxiliary space: $\Theta(1)$.
\item Change in grammar size: $|G'| \in \Theta(|G|)$ and $|R'| \in \Theta(|G|)$ and $|V'-\Sigma| \in O(|G|)$.
\end{itemize}

Since $|R'| = \Theta(|G'|)$, we will use $|R'|$ as a measure of grammar size in subsequent steps.

\subsubsection[Eliminating e-rules]{Eliminating $e$-rules}

\begin{enumerate}
\item Remove all rules of the form $A \gArrow{G} e$.
\item Find $\mathcal{E}$, the set of erasable non-terminals.
\item For every rule $A \garrow{G} BC$,
    if $B \in \mathcal{E}$ then add the rule $A \garrow{G} C$
    and if $C \in \mathcal{E}$ then add the rule $A \garrow{G} B$.
\item If $S \in \mathcal{E}$, add the rule $S \garrow{G} e$.
\end{enumerate}

\begin{itemize}
\item Running time: $O(|V-\Sigma||G|)$.
\item Auxiliary space: $\Theta(|V-\Sigma|)$.
\item Side-effects on grammar: Some non-terminals can become non-instantiable.
\item Change in grammar's size: $|R| \le |R'| \le 3|R|-1$.
\end{itemize}

\subsubsection{Eliminating unit rules}

\begin{enumerate}
\item Find $\operatorname{D}(A) = \{B \in V: A \gArrow{G}^* B\}$ for each $A \in V-\Sigma$.
    This can be done easily since there are no $e$-rules.
\item Remove all rules of the form $A \garrow{G} B$.
\item For all $A \in V - \Sigma$, add rule $A \garrow{G} a$ for $a \in \Sigma \cap \operatorname{D}(A)$
    and add rule $A \Rightarrow CD$ for all $B \in \operatorname{D}(A)$ and $B \garrow{G} CD$.
\end{enumerate}

\begin{itemize}
\item Running time: $O(|V-\Sigma|(|\Sigma| + |R|)$.
\item Auxiliary space: $O(|V||V-\Sigma|)$.
\item Side-effects on grammar: Some non-terminals can become non-instantiable.
\item Change in grammar's size: $|R'| \le |V-\Sigma|\min(|R|, |V|^2)$
    because each new rule's LHS is a non-terminal and each new rule's RHS is the RHS of some old rule.
\end{itemize}

\subsubsection{Eliminating unit rules (alternate algorithm)}

This algorithm has a worse time-complexity than the previous algorithm,
but it is mentioned here for completeness.

\begin{enumerate}
\item Find $\operatorname{D}(A) = \{B \in V: A \gArrow{G}^* B\}$ for each $A \in V-\Sigma$.
    This can be done easily since there are no $e$-rules.
\item Remove all rules of the form $A \garrow{G} B$.
\item For each rule $A \garrow{G} BC$, add the rule
    $A \garrow{G} B_iC_i$ for each $B_i \in \operatorname{D}(B)$ and $C_i \in \operatorname{D}(C)$.
\item For each rule of the form $A \garrow{G} BC$, if $A \in \operatorname{D}(S)$,
    add $S \garrow{G} BC$.
\item Add the rule $S \garrow{G} a$ for all $a \in \Sigma \cap \operatorname{D}(S)$.
\end{enumerate}

\begin{itemize}
\item Running time: $O(|V|(|V-\Sigma|^2 + |R||V|))$.
\item Auxiliary space: $O(|V||V-\Sigma|)$.
\item Side-effects on grammar: Some non-terminals can become non-instantiable.
\item Change in grammar's size: $|R'| \le |V-\Sigma||V|^2$.
\end{itemize}

\subsubsection{Converting to Chomsky normal form}

\begin{itemize}
\item Running time: $O(|G|^2)$.
\item Auxiliary space: $O(|G|^2)$.
\item Change in grammar size: $|V'-\Sigma| \in O(|G|)$, $|R'| \in O(|G|^2)$ and $|G'| \in O(|G|^2)$.
\end{itemize}

\subsection{CYK Algorithm}

The CYK algorithm takes $(w, G)$ as input and uses dynamic programming to find out whether $w \in L(G)$.
Here $G = (V, \Sigma, R, S)$ is a grammar in Chomsky-normal form.

Let $R_A = \{\beta \in V^*: (A \garrow{G} \beta) \in R\}$.
Therefore, $|R| = \sum_{A \in V-\Sigma} |R_A|$.

Let $f(A, i, j) = (A \gArrow{G}^* w[i:j])$. Then
\[ f(A, i, j) = \left\{\begin{array}{cl}
\textrm{false} & i \ge j
\\ w[i] \in R_A & j = i+1
\\ \displaystyle \bigvee\limits_{BC \in R_A} \; \bigvee\limits_{k=i+1}^{j-1}
    (f(B, i, k) \wedge f(C, k, j)) & j > i+1
\end{array}\right. \]

$w \in L(G) \iff f(S, 0, |w|)$.
The CYK algorithm computes $f(A, i, j)$ for all $A \in V-\Sigma$
and all $0 \le i < j \le n$ using dynamic programming.

Time to compute $f(A, i, j)$ is $\Theta(|R_A|(j-i))$
if sub-problem solutions are already available.
Time to compute $f$ for all $(A, i, j)$ is therefore $\Theta(|R||w|^3)$.

% TODO: Also remove non-instantiable or non-reachable non-terminals.

\section{Deterministic Pushdown Automata}

\begin{definition}
Two strings are \emph{consistent} iff one is a prefix of the other.
\end{definition}
\begin{definition}
For a pushdown automaton, two transitions $((p, a_1, \beta_1), (q_1, \gamma_1))$
and $((p, a_2, \beta_2), (q_2, \gamma_2))$ are \emph{compatible} iff
$a_1$ and $a_2$ are consistent and $\beta_1$ and $\beta_2$ are consistent.
\end{definition}
\begin{definition}
A pushdown automaton is \emph{deterministic} iff no two of its transitions are compatible.
\end{definition}
\begin{definition}
A language $L$ is deterministic context-free iff there is a DPDA $M$
such that $L\$ = L(M)$. Here $\$$ is a symbol which is not in the alphabet of $L$.
\end{definition}

\begin{theorem}
\label{halt-properties}
For every DCFL $L$,
there is a DPDA with these properties:
\begin{itemize}
\item It halts on an input iff it reads the whole input.
\item It has an empty stack and is not in the starting state iff it has halted.
\end{itemize}
\end{theorem}
From now on we can (and we will) assume (WLOG) that all DPDAs have the above properties.

\subsection{Closure under complementation}

\begin{theorem}
\label{dpda-complement}
Let $\overline{M}$ be the DPDA obtained by switching the finalness of states of DPDA $M$.
Then $w$ halts $\overline{M}$ iff it halts $M$.
Also, if $w$ halts $M$, $w \in L(\overline{M}) \iff w \in \overline{L(M)}$.
\end{theorem}

\begin{definition}
The configuration $(p, w, x)$ is a dead-end iff
$(p, w, x) \vdash_M^* (q, w', y) \implies (w = w' \wedge |x| \le |y|)$.
\end{definition}

\begin{theorem}
DPDA $M$ does not halt on input $w$ iff it has a dead-end configuration.
\end{theorem}

\begin{theorem}
Every DPDA has an equivalent simple DPDA.
\end{theorem}

If $(q, w, x)$ is a dead end in a simple DPDA, then by theorem \ref{halt-properties},
$w \neq e$ and $x \neq e$.
\begin{theorem}
$(q, aw, bx)$ is a dead end iff $(q, a, b)$ is also a dead end.
\end{theorem}

Let $D \subseteq Q \times \Sigma \times \Gamma$ be the set of all dead-end triples.
This is a well-defined and finite set.

Steps to convert a simple DPDA $M$ into an equivalent simple DPDA $M'$ without dead-ends:
\begin{itemize}
\item For all $(q, a, b) \in D$, remove from $\Delta$ all transitions compatible with $(q, a, b)$.
\item Add 2 non-final states $r$ and $r'$.
\item For all $(q, a, b) \in D$, add the transition $((q, a, b), (r, e))$.
\item Add the transitions $((r, a, e), (r, e))$ for all $a \in \Sigma$.
\item Add the transition $((r, \$, e), (r', e))$.
\item Add the transitions $((r', e, b), (r', e))$ for all $b \in \Gamma$
    (by the way, $\Gamma$ includes the bottom-marker).
\end{itemize}

If $M$ has no dead-end configurations, $L(\overline{M}) = \overline{L(M)}$.
Therefore, deterministic context-free languages are closed under complementation.

\begin{theorem}
Let $L = \{a^mb^nc^p: m \neq n \vee n \neq p\}$. Then $L$ is context free but not deterministic context-free.
\end{theorem}
\begin{proof}
$L' = \overline{L}\cap a^*b^*c^* = \{a^nb^nc^n: n \ge 0\}$ is known to not be context-free.
If $L$ was DCFL, $\overline{L}$ would be a DCFL and so $L'$ would be a CFL.
\end{proof}

\section{Deterministic Parsing}

\begin{definition}
For a grammar $G$, a deterministic parser $M$ is a DPDA which can
identify whether $w \in L(G)$ and construct a parse-tree for $w$ in $G$.
It does this by sometimes outputting a rule when following a transition.
These rules, taken in order, form the rules applied in a leftmost derivation of $w$.
\end{definition}

\subsection{Single look-ahead parser}

\begin{definition}
For grammar $G = (V, \Sigma, R, S)$, a single look-ahead parser is a DPDA
$M = (\{p, q\}\cup\{q_a: a \in \Sigma \cup \{\$\}\}, \Sigma, V, \Delta, p, \{q_{\$}\})$
such that $\Delta$ has the following transitions:
\begin{itemize}
\item $((p, e, e), (q, S))$
\item $((q, a, e), (q_a, e))$ for all $a \in \Sigma \cup \{\$\}$
\item $((q_a, e, a), (q, e))$ for all $a \in \Sigma \cup \{\$\}$
\item $((q_a, e, A), (q_a, \beta))$ for all $A \garrow{G} B \in R$ for some $a \in \Sigma \cup \{\$\}$.
    Output $A \garrow{G} B$ when following this transition.
\end{itemize}
For $M$ to be deterministic, for transitions $((q_a, e, A), (q_a, \beta_1))$
and $((q_b, e, A), (q_b, \beta_2))$, $a \neq b$.
\end{definition}

Not all grammars have a single look-ahead parser.
But it is sometimes possible to transform a grammar so that it has a single look-ahead parser.
We now mention two heuristic rules for such transformations.

\subsubsection{Heuristic transformation rules}

\paragraph{Left factoring.}
If there are rules $A \rightarrow \alpha\beta_i$ for multiple $i$,
transform them to rules $A \rightarrow \alpha B$ and $B \rightarrow \beta_i$ for all $i$.

\paragraph{Removing left recursion.}
If there are rules $A \rightarrow A\alpha_i$ and $A \rightarrow \beta_j$
for $i \ge 1$ and $j \ge 1$,
transform them to rules $A \rightarrow \beta_jB$, $B \rightarrow e$ and $B \rightarrow \alpha_iB$
for $i \ge 1$ and $j \ge 1$.

\end{document}
